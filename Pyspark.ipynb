{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/7oda111/Data-Integration-Tool-/blob/main/Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13327d7d-2301-4fa8-bd17-15208d207790",
      "metadata": {
        "id": "13327d7d-2301-4fa8-bd17-15208d207790"
      },
      "source": [
        "# Jupyter Notebook: Introduction to PySpark for BI Developers\n",
        "\n",
        "Below is a markdown-based Jupyter Notebook guide for a session on **PySpark** and its background, history, use cases, and examples. You can copy the entire content into a `.ipynb` file or directly into a single Jupyter Notebook cell set to Markdown.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61f160b5-5e5c-4b0d-b85b-8989f6c0ea16",
      "metadata": {
        "id": "61f160b5-5e5c-4b0d-b85b-8989f6c0ea16"
      },
      "source": [
        "## Table of Contents\n",
        "1. [PySpark Background and History](#pyspark-background)\n",
        "2. [Foundations and Architecture](#pyspark-foundations)\n",
        "3. [Key PySpark Data Structures](#pyspark-data-structures)\n",
        "4. [Use Cases for BI Developers](#pyspark-use-cases)\n",
        "5. [Example Code Snippets](#pyspark-code-examples)\n",
        "6. [Test Cases and Scenarios](#pyspark-test-cases)\n",
        "7. [Conclusion and Q&A](#conclusion)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1e20903-a04c-4d17-8b2a-ee923a9d1069",
      "metadata": {
        "id": "c1e20903-a04c-4d17-8b2a-ee923a9d1069"
      },
      "source": [
        "<a id=\"pyspark-background\"></a>\n",
        "## 1. PySpark Background and History\n",
        "\n",
        "### Spark Overview\n",
        "- **Apache Spark** was developed at UC Berkeley’s AMPLab in 2009.\n",
        "- Spark quickly became a top-level project at the Apache Software Foundation due to its high performance and ease of use.\n",
        "- Spark provides a **unified analytics engine** for large-scale data processing.\n",
        "\n",
        "### PySpark\n",
        "- **PySpark** is the **Python API** for Apache Spark.\n",
        "- Allows **Python developers** to leverage Spark’s capabilities in a familiar language.\n",
        "- Enables scalable data processing, interactive DataFrames, MLlib integration, and more.\n",
        "\n",
        "### Why Python?\n",
        "- Python’s readability and wide user base.\n",
        "- Rich ecosystem of data analysis libraries (pandas, NumPy, etc.) which can integrate well with PySpark.\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"pyspark-foundations\"></a>\n",
        "## 2. Foundations and Architecture\n",
        "\n",
        "### Spark Architecture\n",
        "- **Driver**: Coordinates the application (runs the user’s main function, creates RDDs/DataFrames, and schedules tasks).\n",
        "- **Cluster Manager**: Allocates resources across the cluster (e.g., Standalone, YARN, Kubernetes).\n",
        "- **Executors**: Processes tasks and stores data on worker nodes.\n",
        "\n",
        "### RDDs and DataFrames\n",
        "- **RDD (Resilient Distributed Dataset)**: Low-level API for distributed data.\n",
        "- **DataFrame**: Higher-level abstraction with schema, enabling SQL-like queries.\n",
        "- **Dataset** (Scala/Java-specific typed API).\n",
        "\n",
        "### Core Concepts\n",
        "- **Transformations vs Actions**:\n",
        "  - **Transformations** (e.g., `map`, `filter`) define the computation.\n",
        "  - **Actions** (e.g., `collect`, `count`) trigger execution.\n",
        "- **Lazy Evaluation**: Transformations do not compute immediately, allowing Spark to optimize.\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"pyspark-data-structures\"></a>\n",
        "## 3. Key PySpark Data Structures\n",
        "\n",
        "### RDD Example\n",
        "```python\n",
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext(\"local\", \"MyApp\")\n",
        "rdd = sc.parallelize([1, 2, 3, 4])\n",
        "print(rdd.map(lambda x: x * 2).collect())  # [2, 4, 6, 8]\n",
        "```\n",
        "\n",
        "### DataFrame Example\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"MySQLApp\").getOrCreate()\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
        "df.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"pyspark-use-cases\"></a>\n",
        "## 4. Use Cases for BI Developers\n",
        "\n",
        "1. **ETL Pipelines**\n",
        "   - Reading large datasets from data lakes (e.g., HDFS, S3).\n",
        "   - Performing transformations and aggregations.\n",
        "   - Loading into data warehouses or analytical databases.\n",
        "2. **Interactive Analytics**\n",
        "   - Using DataFrames to quickly query data.\n",
        "   - Integrating PySpark with Jupyter for an interactive experience.\n",
        "3. **Real-Time Streaming**\n",
        "   - Leveraging Structured Streaming to process logs, clickstreams, or IoT data.\n",
        "4. **Machine Learning**\n",
        "   - Using MLlib for model training at scale.\n",
        "5. **Data Integration**\n",
        "   - Combining multiple sources (RDBMS, CSV, Parquet, NoSQL) into a single pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"pyspark-code-examples\"></a>\n",
        "## 5. Example Code Snippets\n",
        "\n",
        "### Example 1: Simple Word Count\n",
        "```python\n",
        "# Initialize SparkContext or SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"WordCount\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "text_rdd = sc.textFile(\"sample_text.txt\")\n",
        "word_count = (text_rdd\n",
        "              .flatMap(lambda line: line.split())\n",
        "              .map(lambda word: (word, 1))\n",
        "              .reduceByKey(lambda a, b: a + b))\n",
        "\n",
        "for word, count in word_count.collect():\n",
        "    print(word, count)\n",
        "```\n",
        "\n",
        "### Example 2: DataFrame Aggregation\n",
        "```python\n",
        "# Create a DataFrame from a list of tuples\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    (\"Alice\", \"Sales\", 1000),\n",
        "    (\"Bob\",   \"Sales\",  800),\n",
        "    (\"Cathy\", \"IT\",    1300),\n",
        "    (\"Dave\",  \"IT\",    1500)\n",
        "], [\"Name\", \"Department\", \"Salary\"])\n",
        "\n",
        "# Calculate average salary by department\n",
        "avg_df = df.groupBy(\"Department\").agg(avg(\"Salary\").alias(\"AvgSalary\"))\n",
        "avg_df.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"pyspark-test-cases\"></a>\n",
        "## 6. Test Cases and Scenarios\n",
        "\n",
        "1. **Data Consistency Check**\n",
        "   - Scenario: You have a DataFrame with duplicate records.\n",
        "   - Test: Use `dropDuplicates()` and compare the row counts before and after.\n",
        "\n",
        "2. **Schema Validation**\n",
        "   - Scenario: The schema (columns, data types) might change unexpectedly.\n",
        "   - Test: Check if the schema matches expected structure, raise alerts if not.\n",
        "\n",
        "3. **Performance Benchmark**\n",
        "   - Scenario: Large dataset (in the range of millions of rows).\n",
        "   - Test: Measure execution time before and after optimizing partitions and caching.\n",
        "\n",
        "4. **Edge Cases**\n",
        "   - Empty files, missing columns, or partially corrupted data.\n",
        "   - Test: Validate behaviors with empty RDDs/DataFrames.\n",
        "\n",
        "5. **Integration with Data Warehouses**\n",
        "   - Scenario: Writing the final aggregated data to a data warehouse table.\n",
        "   - Test: Ensure the write operations are successful, data is in correct format.\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"conclusion\"></a>\n",
        "## 7. Conclusion and Q&A\n",
        "\n",
        "- **Key Takeaways**:\n",
        "  1. PySpark provides a **scalable** and **flexible** platform for BI developers.\n",
        "  2. **RDDs** and **DataFrames** allow you to process structured and unstructured data.\n",
        "  3. Real-time streaming capabilities can handle **continuous data pipelines**.\n",
        "  4. Integrates seamlessly with **machine learning** and other Big Data tools.\n",
        "\n",
        "- **Next Steps**:\n",
        "  - Explore **Structured Streaming** for real-time pipelines.\n",
        "  - Dive into **Spark MLlib** for end-to-end machine learning.\n",
        "  - Implement best practices: partitioning, caching, and broadcast joins.\n",
        "\n",
        "---\n",
        "\n",
        "### Q&A\n",
        "Feel free to ask any questions regarding setup, optimization, best practices, or real-time analytics with PySpark!\n",
        "\n",
        "---\n",
        "\n",
        "## Additional References\n",
        "- [Apache Spark Official Documentation](https://spark.apache.org/docs/latest/)\n",
        "- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/index.html)\n",
        "- [Databricks Tutorials](https://docs.databricks.com/getting-started/index.html)\n",
        "\n",
        "**Thank You!**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "859b91be-009d-44b8-a9be-95332790af0d",
      "metadata": {
        "id": "859b91be-009d-44b8-a9be-95332790af0d"
      },
      "source": [
        "## Spark vs. Pandas\n",
        "\n",
        "| Aspect                | Spark (PySpark)                                         | Pandas                                             |\n",
        "|-----------------------|---------------------------------------------------------|----------------------------------------------------|\n",
        "| **Data Handling**     | Distributed data processing across a cluster           | Single machine, in-memory data processing         |\n",
        "| **Scalability**       | Scales to petabytes of data and thousands of nodes      | Limited by local system’s RAM                     |\n",
        "| **Execution Model**   | Lazy evaluation, transformations and actions            | Eager evaluation (operations executed immediately)|\n",
        "| **Performance**       | Designed for high throughput and parallel processing    | Fast for small to medium data, limited by memory  |\n",
        "| **Use Cases**         | Big data, ETL pipelines, streaming, machine learning    | Interactive data analysis, prototyping, data wrangling |\n",
        "| **Fault Tolerance**   | Automatic data replication and fault tolerance          | Minimal fault tolerance; relies on OS environment |\n",
        "| **API Level**         | DataFrame and RDD APIs with SQL-like operations         | DataFrame-based, Python-centric approach           |\n",
        "\n",
        "### How Spark Can Enhance BI Skills\n",
        "1. **Handling Larger Datasets**  \n",
        "   - As a BI developer, you often work with large, rapidly growing data sources. Spark’s **distributed nature** allows you to process datasets far exceeding the capacity of a single machine.\n",
        "\n",
        "2. **Real-Time Analytics**  \n",
        "   - Spark’s **Structured Streaming** integrates seamlessly with Kafka, enabling **real-time dashboards** and immediate data insights—a critical advantage for data-driven decision making in BI.\n",
        "\n",
        "3. **Scalable ETL Pipelines**  \n",
        "   - Spark’s ability to **distribute ETL tasks** across a cluster means faster transformations and **shorter processing times**, crucial for keeping BI reports and dashboards updated.\n",
        "\n",
        "4. **Unified Analytics Engine**  \n",
        "   - Whether you need SQL-like queries, machine learning, or streaming analytics, Spark offers a **unified platform**. This broadens your BI capabilities beyond traditional batch processing.\n",
        "\n",
        "5. **Seamless Integration**  \n",
        "   - Spark integrates with **data warehouses, cloud storage, NoSQL databases,** and other big data tools. This helps BI developers create **end-to-end pipelines** for comprehensive analytics.\n",
        "\n",
        "6. **Skill Set Expansion**  \n",
        "   - Mastering Spark positions BI developers to handle **big data challenges**, bridging the gap between traditional analytics and large-scale, distributed data solutions.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70f0bbd5-5fbe-4119-b996-761245e33354",
      "metadata": {
        "id": "70f0bbd5-5fbe-4119-b996-761245e33354"
      },
      "source": [
        "# PySpark Testing Guide (Markdown)\n",
        "\n",
        "Below is a comprehensive Markdown guide containing various PySpark code snippets and explanations. You can copy this into a Jupyter Notebook cell (set as Markdown) or a `.md` file and test each code block in a PySpark environment.\n",
        "\n",
        "---\n",
        "## Table of Contents\n",
        "1. [Setup and Environment](#setup)\n",
        "2. [Creating a Spark Session](#spark-session)\n",
        "3. [RDD Operations](#rdd-operations)\n",
        "4. [DataFrame Operations](#df-operations)\n",
        "5. [DataFrame Transformations](#df-transformations)\n",
        "6. [Spark SQL](#spark-sql)\n",
        "7. [Reading and Writing Data](#io-operations)\n",
        "8. [Performance Tips](#performance)\n",
        "9. [Testing and Validation](#testing)\n",
        "\n",
        "---\n",
        "<a id=\"setup\"></a>\n",
        "## 1. Setup and Environment\n",
        "\n",
        "### Prerequisites\n",
        "- Python 3.x\n",
        "- Java 8 or later\n",
        "- Spark binaries (or use PySpark installed via `pip install pyspark`)\n",
        "\n",
        "### Verifying Installation\n",
        "```bash\n",
        "# Check Java version\n",
        "java -version\n",
        "\n",
        "# Check Python version\n",
        "python --version\n",
        "\n",
        "# If installing PySpark via pip\n",
        "pip install pyspark\n",
        "```\n",
        "\n",
        "---\n",
        "<a id=\"spark-session\"></a>\n",
        "## 2. Creating a Spark Session\n",
        "\n",
        "To test any PySpark code, you need a **SparkSession**. You can create one by:\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder\n",
        "    .appName(\"PySparkTesting\")\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark version:\", spark.version)\n",
        "```\n",
        "\n",
        "- **Output**: You should see the Spark version printed.\n",
        "\n",
        "---\n",
        "<a id=\"rdd-operations\"></a>\n",
        "## 3. RDD Operations\n",
        "\n",
        "### Creating an RDD\n",
        "```python\n",
        "from pyspark import SparkContext\n",
        "\n",
        "# If you need a SparkContext, retrieve it from SparkSession\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Create an RDD from a Python list\n",
        "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
        "print(\"RDD Partitions:\", rdd.getNumPartitions())\n",
        "```\n",
        "- **Test**: The output should show the number of partitions allocated.\n",
        "\n",
        "### Transformations and Actions\n",
        "```python\n",
        "# Map transformation\n",
        "mapped_rdd = rdd.map(lambda x: x * 2)\n",
        "\n",
        "# Filter transformation\n",
        "filtered_rdd = mapped_rdd.filter(lambda x: x > 5)\n",
        "\n",
        "# Collect action\n",
        "result = filtered_rdd.collect()\n",
        "print(\"Transformed RDD:\", result)\n",
        "```\n",
        "- **Expected**: `[6, 8, 10]` (doubled values above 5).\n",
        "\n",
        "---\n",
        "<a id=\"df-operations\"></a>\n",
        "## 4. DataFrame Operations\n",
        "\n",
        "### Creating a DataFrame\n",
        "```python\n",
        "# Create a DataFrame from a Python list of tuples\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Cathy\", 29)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()\n",
        "```\n",
        "- **Expected Output**: A table with Name and Age columns.\n",
        "\n",
        "### Schema Inspection\n",
        "```python\n",
        "df.printSchema()\n",
        "```\n",
        "- **Expected**:\n",
        "  ```\n",
        "  root\n",
        "   |-- Name: string (nullable = true)\n",
        "   |-- Age: long (nullable = true)\n",
        "  ```\n",
        "\n",
        "---\n",
        "<a id=\"df-transformations\"></a>\n",
        "## 5. DataFrame Transformations\n",
        "\n",
        "### Select and Filter\n",
        "```python\n",
        "# Select a subset of columns\n",
        "df_selected = df.select(\"Name\")\n",
        "df_selected.show()\n",
        "\n",
        "# Filter rows\n",
        "df_filtered = df.filter(df.Age > 25)\n",
        "df_filtered.show()\n",
        "```\n",
        "- **Expected**: Only Bob (30) and Cathy (29) in the filtered output.\n",
        "\n",
        "### Adding a Column\n",
        "```python\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "df_with_constant = df.withColumn(\"Country\", lit(\"USA\"))\n",
        "df_with_constant.show()\n",
        "```\n",
        "- **Expected**: A new column named Country with value \"USA\".\n",
        "\n",
        "### Aggregation\n",
        "```python\n",
        "from pyspark.sql.functions import avg, count\n",
        "\n",
        "agg_df = df.agg(avg(\"Age\").alias(\"AverageAge\"), count(\"Name\").alias(\"NameCount\"))\n",
        "agg_df.show()\n",
        "```\n",
        "- **Expected**: A single row with average age and name count.\n",
        "\n",
        "---\n",
        "<a id=\"spark-sql\"></a>\n",
        "## 6. Spark SQL\n",
        "\n",
        "### Register a Temporary View\n",
        "```python\n",
        "df.createOrReplaceTempView(\"people\")\n",
        "\n",
        "# Run SQL queries\n",
        "sql_df = spark.sql(\"SELECT Name, Age FROM people WHERE Age > 25\")\n",
        "sql_df.show()\n",
        "```\n",
        "- **Expected**: Rows for Bob and Cathy.\n",
        "\n",
        "### Join Example\n",
        "```python\n",
        "department_data = [\n",
        "    (\"Sales\", \"Alice\"),\n",
        "    (\"Sales\", \"Bob\"),\n",
        "    (\"IT\", \"Cathy\"),\n",
        "]\n",
        "\n",
        "columns_dept = [\"Department\", \"Name\"]\n",
        "df_dept = spark.createDataFrame(department_data, columns_dept)\n",
        "df_dept.createOrReplaceTempView(\"dept\")\n",
        "\n",
        "join_df = spark.sql(\"\"\"\n",
        "    SELECT p.Name, p.Age, d.Department\n",
        "    FROM people p\n",
        "    JOIN dept d ON p.Name = d.Name\n",
        "\"\"\")\n",
        "\n",
        "join_df.show()\n",
        "```\n",
        "- **Expected**: Rows linking each person to their department.\n",
        "\n",
        "---\n",
        "<a id=\"io-operations\"></a>\n",
        "## 7. Reading and Writing Data\n",
        "\n",
        "### Reading CSV\n",
        "```python\n",
        "# Example CSV file path (you need a real path on your machine)\n",
        "csv_path = \"path/to/people.csv\"\n",
        "csv_df = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .csv(csv_path)\n",
        "csv_df.show()\n",
        "```\n",
        "- **Replace** `\"path/to/people.csv\"` with a valid CSV file path.\n",
        "\n",
        "### Writing Parquet\n",
        "```python\n",
        "output_path = \"path/to/output/parquet\"\n",
        "csv_df.write.mode(\"overwrite\").parquet(output_path)\n",
        "```\n",
        "- **Test**: Check the output directory for Parquet files.\n",
        "\n",
        "---\n",
        "<a id=\"performance\"></a>\n",
        "## 8. Performance Tips\n",
        "\n",
        "1. **Partitions**: Increase or decrease partitions based on data size.\n",
        "2. **Broadcast Joins**: Use `broadcast()` when one DataFrame is small.\n",
        "3. **Caching**: Cache DataFrames to memory if reused.\n",
        "4. **Predicate Pushdown**: Make use of DataFrame filters early.\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "small_df = spark.createDataFrame([(\"key1\", 100), (\"key2\", 200)], [\"Key\", \"Value\"])\n",
        "large_df = spark.range(0, 100000).withColumnRenamed(\"id\", \"Key\")\n",
        "\n",
        "joined_df = large_df.join(broadcast(small_df), on=\"Key\", how=\"inner\")\n",
        "```\n",
        "\n",
        "---\n",
        "<a id=\"testing\"></a>\n",
        "## 9. Testing and Validation\n",
        "\n",
        "### Example: Asserting DataFrame Counts\n",
        "\n",
        "```python\n",
        "result_count = df.count()\n",
        "expected_count = 3\n",
        "assert result_count == expected_count, f\"Expected {expected_count}, got {result_count}\"\n",
        "print(\"DataFrame count test passed!\")\n",
        "```\n",
        "\n",
        "### Example: Checking Schema\n",
        "```python\n",
        "actual_schema = df.schema.simpleString()\n",
        "expected_schema = \"struct<Name:string,Age:bigint>\"\n",
        "assert actual_schema == expected_schema, f\"Schema mismatch. Got {actual_schema}\"\n",
        "print(\"Schema test passed!\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Final Notes\n",
        "- In a **Jupyter Notebook**, split each code snippet into separate code cells.\n",
        "- Make sure the file paths for input/output in the I/O operations code are valid on your system.\n",
        "- You can stop the Spark session once you’re done:\n",
        "  ```python\n",
        "  spark.stop()\n",
        "  ```\n",
        "- For more advanced testing frameworks, consider using **`pytest`** along with **`pytest-spark`** or **`unittest`**.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Happy Testing with PySpark!**\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}